{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOo3Y26RBivGsNVvT/5f4mx",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/leomoritz/bigdata-pyspark/blob/master/pythonspark.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Prática 01: Configuração do Ambiente**"
      ],
      "metadata": {
        "id": "AZ0zKoCdzpzy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Instale a linguagem Java 8, que é o pré-requisito para usar o Apache Spark:"
      ],
      "metadata": {
        "id": "TBux6Iua0lRF"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "xTu-8jaozTAD"
      },
      "outputs": [],
      "source": [
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Instale o Apache Spark com Hadoop, executando o seguinte comando:"
      ],
      "metadata": {
        "id": "cvxJY9XQ0yWN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget -q https://downloads.apache.org/spark/spark-3.3.3/spark-3.3.3-bin-hadoop3.tgz"
      ],
      "metadata": {
        "id": "Jn78NLnN0YuB"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!tar xf spark-3.3.3-bin-hadoop3.tgz"
      ],
      "metadata": {
        "id": "M8FTPs3oOaOR"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Para instalar o FindSpark (adiciona o PySpark no caminho do sistema sys.path em tempo de execução), use o seguinte comando:"
      ],
      "metadata": {
        "id": "pX1mq4hMzobe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q findspark"
      ],
      "metadata": {
        "id": "Vn_5qIE81PtG"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Verificando se o findspark foi instalado com sucesso:"
      ],
      "metadata": {
        "id": "EgXdHeip2sfE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "help()"
      ],
      "metadata": {
        "id": "qyhQOrux2v27"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Instalando o PySpark:"
      ],
      "metadata": {
        "id": "-tLB-yXA1iIQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q pyspark"
      ],
      "metadata": {
        "id": "BLTQuIlu1k5M",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "703d7366-d4d3-4483-e91e-3edc85b4dec9"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m316.9/316.9 MB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Salvando as variáveis de ambiente"
      ],
      "metadata": {
        "id": "KI_c5mjv1phe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.3.3-bin-hadoop3\""
      ],
      "metadata": {
        "id": "ajPIFaKy1zI6"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Prática 02: Começando a trabalhar com Spark**"
      ],
      "metadata": {
        "id": "dajok_j83vDs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Importando e iniciando o framework do findspark"
      ],
      "metadata": {
        "id": "NYMVfid56Ud9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import findspark as fs\n",
        "fs.init()"
      ],
      "metadata": {
        "id": "bgGEsybW30Xs"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark import SparkContext\n",
        "spark_context = SparkContext()"
      ],
      "metadata": {
        "id": "7BKr5NSd8xV1"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Iniciando sessão do spark local para criar uma instância caso não exista."
      ],
      "metadata": {
        "id": "3qqjoHgm6aJr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.master(\"local[*]\").getOrCreate()"
      ],
      "metadata": {
        "id": "ExwiHSJO4LgR"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Salvando os dados de um CSV dentro de um dataset."
      ],
      "metadata": {
        "id": "UHSvwMB76sT7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = spark.read.csv('/content/sample_data/california_housing_test.csv', inferSchema=True, header=True)"
      ],
      "metadata": {
        "id": "loGs5noK52sm"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Visualização dos dados"
      ],
      "metadata": {
        "id": "8q1A2p2y66Hr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset.printSchema()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MH3goIwu7DIN",
        "outputId": "f4a17fd7-c769-474d-d7f0-57bc6253aab6"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- longitude: double (nullable = true)\n",
            " |-- latitude: double (nullable = true)\n",
            " |-- housing_median_age: double (nullable = true)\n",
            " |-- total_rooms: double (nullable = true)\n",
            " |-- total_bedrooms: double (nullable = true)\n",
            " |-- population: double (nullable = true)\n",
            " |-- households: double (nullable = true)\n",
            " |-- median_income: double (nullable = true)\n",
            " |-- median_house_value: double (nullable = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z8USzVyASZHG",
        "outputId": "eff02ccd-efb3-49ba-b466-64d374cc7579"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Row(longitude=-122.05, latitude=37.37, housing_median_age=27.0, total_rooms=3885.0, total_bedrooms=661.0, population=1537.0, households=606.0, median_income=6.6085, median_house_value=344700.0)"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset.count()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AV5vu1O45P6l",
        "outputId": "339c4bea-1488-4fce-b1c5-ad4199b5598e"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3000"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Encerrando o spark"
      ],
      "metadata": {
        "id": "DCnlwjlu7mLR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "spark.stop()"
      ],
      "metadata": {
        "id": "ZQDlp8Mm7oBk"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Prática 3 - Spark no Python"
      ],
      "metadata": {
        "id": "N5ywrXKLa0SP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import findspark as fs\n",
        "fs.init()"
      ],
      "metadata": {
        "id": "w6HnjC6zcvW4"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark import SparkContext\n",
        "spark_context = SparkContext()\n",
        "print(spark_context)\n",
        "print(spark_context.version)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nbtL4eTlcaWz",
        "outputId": "a5b0c27b-cae1-4251-abe3-13e1d2b429d9"
      },
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<SparkContext master=local[*] appName=pyspark-shell>\n",
            "3.3.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.getOrCreate()\n",
        "print(spark)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TQfyPVf0dNH8",
        "outputId": "a04a1d72-3bd3-4afd-ee0e-6500ebc24824"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<pyspark.sql.session.SparkSession object at 0x7dad2414d450>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = spark.read.csv('/content/sample_data/california_housing_test.csv', inferSchema=True, header=True)"
      ],
      "metadata": {
        "id": "rzbd5fB0dfcY"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CAwOjPqpdy9B",
        "outputId": "60cc3e00-429e-48c9-e374-a42b79541f42"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Row(longitude=-122.05, latitude=37.37, housing_median_age=27.0, total_rooms=3885.0, total_bedrooms=661.0, population=1537.0, households=606.0, median_income=6.6085, median_house_value=344700.0)"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset.createOrReplaceTempView(\"tabela_temporaria\")\n",
        "print(spark.catalog.listTables())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IdV4sN3Nd1gc",
        "outputId": "72bbee04-9d75-466d-b685-60d6155ab87d"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Table(name='tabela_temporaria', database=None, description=None, tableType='TEMPORARY', isTemporary=True)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"FROM tabela_temporaria SELECT longitude, latitude LIMIT 3\"\n",
        "saida = spark.sql(query)\n",
        "saida.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oht22WqSeEWF",
        "outputId": "7fd3be62-f375-4579-dfa4-d0b4910e8657"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+--------+\n",
            "|longitude|latitude|\n",
            "+---------+--------+\n",
            "|  -122.05|   37.37|\n",
            "|   -118.3|   34.26|\n",
            "|  -117.81|   33.78|\n",
            "+---------+--------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Prática 4 - Spark com Pandas"
      ],
      "metadata": {
        "id": "y78hPbPKgLEX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Implementar a consulta SQL na tabela chamada de tabela_temporaria, que já carregamos no Spark para retornar a quantidade máxima de quartos;"
      ],
      "metadata": {
        "id": "i0XNkxDqhQiZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "query1 = \"SELECT MAX(total_rooms) as maximo_quartos FROM tabela_temporaria\""
      ],
      "metadata": {
        "id": "KUGARFyzgQaR"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Executar a consulta SQL no Spark e, assim, obter um DataFrame do Spark;"
      ],
      "metadata": {
        "id": "sZsXaS3mhXD2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "q_maximo_quartos = spark.sql(query1)"
      ],
      "metadata": {
        "id": "qfpXHvtOhZ68"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Converter o resultado da etapa anterior para um DataFrame do Pandas;"
      ],
      "metadata": {
        "id": "4vZaDumShas3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pd_maximo_quartos = q_maximo_quartos.toPandas()"
      ],
      "metadata": {
        "id": "Ul6XL6ojhc5Q"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Imprimir o resultado da consulta;"
      ],
      "metadata": {
        "id": "Y62yLKjBheww"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print('A quantidade máxima de quartos é: {}'.format(pd_maximo_quartos))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ySv-iDALhht0",
        "outputId": "2ce62568-5aec-4eed-d88f-a7262ed514e3"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "A quantidade máxima de quartos é:    maximo_quartos\n",
            "0         30450.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Converter o valor do DataFrame para um valor inteiro."
      ],
      "metadata": {
        "id": "iwm_uqUjhlij"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "qtd_maximo_quartos = int(pd_maximo_quartos.loc[0,'maximo_quartos'])"
      ],
      "metadata": {
        "id": "zO79DaRBhlAV"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Implementar a consulta SQL para retornar a latitude e a longitude da residência com a quantidade máxima de quartos que obtivemos na execução do programa anterior;"
      ],
      "metadata": {
        "id": "5IS9l5YUiFtE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "query2 = \"SELECT longitude, latitude FROM tabela_temporaria WHERE total_rooms=\"+str(qtd_maximo_quartos)"
      ],
      "metadata": {
        "id": "SsEDN_rfiGW8"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Executar SQL no Spark e obter o resultado no DataFrame do Spark"
      ],
      "metadata": {
        "id": "ZqKeWdmdj_D3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "localizacao_maximo_quartos = spark.sql(query2)"
      ],
      "metadata": {
        "id": "lNDL-fTEkDvX"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Converter o DataFrame do Spark para o DataFrame do Pandas"
      ],
      "metadata": {
        "id": "HuRNlV1XkPnu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pd_localizacao_maximo_quartos = localizacao_maximo_quartos.toPandas()"
      ],
      "metadata": {
        "id": "TRZChlSckTSK"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Exibir resultado"
      ],
      "metadata": {
        "id": "gt664RNlkUZa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(pd_localizacao_maximo_quartos)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ny8n2pPvkVnB",
        "outputId": "9d6c04b5-8af4-4673-a3e2-9b8e66e3f222"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   longitude  latitude\n",
            "0     -117.2     33.58\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Prática 5 - Convertendo Pandas DataFrame para Spark DataFrame"
      ],
      "metadata": {
        "id": "qBibkNKscWp5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Importando as bibliotecas numpy para gerar os dados e pandas para organizá-los em um dataframe."
      ],
      "metadata": {
        "id": "ZW0LvanYc4aA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "bCR7kZqtcgXe"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Gerando os dados com numpy de forma randomica e armazenando em um dataframe do pandas."
      ],
      "metadata": {
        "id": "RkFp4ntQdAHg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "media = 0\n",
        "desvio_padrao = 0.1\n",
        "pd_temporario = pd.DataFrame(np.random.normal(media, desvio_padrao, 100))"
      ],
      "metadata": {
        "id": "lZGaNAY6dFvC"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Convertendo o dataframe do pandas em dataframe do spark e printando resultado"
      ],
      "metadata": {
        "id": "2_-8etowdcUQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "spark_temporario = spark.createDataFrame(pd_temporario);\n",
        "print(spark.catalog.listTables())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bWDOQHj4dgcJ",
        "outputId": "69172a5b-91b9-43b7-a1d1-8b2e3405d11a"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Table(name='nova_tabela_temporaria', database=None, description=None, tableType='TEMPORARY', isTemporary=True), Table(name='tabela_temporaria', database=None, description=None, tableType='TEMPORARY', isTemporary=True)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Aqui, listamos apenas as tabelas que já estavam no catálogo. Agora listaremos as informações sobre a última tabela que criamos através da função createOrReplaceTempView:"
      ],
      "metadata": {
        "id": "KhRIteqAfw3S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "spark_temporario.createOrReplaceTempView(\"nova_tabela_temporaria\")\n",
        "print(spark.catalog.listTables())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "muzhcgW4fxXQ",
        "outputId": "432df933-1e50-45be-f52c-a0cbd54708da"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Table(name='nova_tabela_temporaria', database=None, description=None, tableType='TEMPORARY', isTemporary=True), Table(name='tabela_temporaria', database=None, description=None, tableType='TEMPORARY', isTemporary=True)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Por fim, encerraremos a sessão:"
      ],
      "metadata": {
        "id": "EWXfMakzgGx8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "spark.stop()"
      ],
      "metadata": {
        "id": "N7gzxDK0gKQ3"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Prática 6 - MapReduce"
      ],
      "metadata": {
        "id": "psKncumsvBcg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Considerando que o contexto já foi instanciado e a biblioteca numpy importada anteriormente, então o primeiro passo aqui é criar um vetor de dados"
      ],
      "metadata": {
        "id": "duFRJACyvI1Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vetor = np.array([10, 20, 30, 40, 50])"
      ],
      "metadata": {
        "id": "_ZrWy-tCvHcc"
      },
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Feito isso, agora o próximo passo a criar um RDD por meio de um SparkContext"
      ],
      "metadata": {
        "id": "GWSxb_uKvIJ3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "paralelo = spark_context.parallelize(vetor)\n",
        "print(paralelo)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SVhdIdAMv-r-",
        "outputId": "a05fd43f-6b4c-4d7d-94d0-c8bd462c9b15"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ParallelCollectionRDD[0] at readRDDFromFile at PythonRDD.scala:274\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "O próximo passo é aplicar o mapeamento através de uma função lambda que pegará cada elemento do vetor (x) e irá elevá-lo ao quadrado e por fim somar a ele mesmo. Ex: 10² + 10 = 110."
      ],
      "metadata": {
        "id": "sS-j9OHav_48"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "mapa = paralelo.map(lambda x : x**2 + x)\n",
        "outro_mapa = paralelo.map(lambda x : x**4-10*x**2+3)"
      ],
      "metadata": {
        "id": "tjPj_8EnwMe2"
      },
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Agora é possível coletar os dados para serem verificados"
      ],
      "metadata": {
        "id": "g79A_dfiwsvb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "mapa.collect()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "56DVvht1ww2D",
        "outputId": "45f02e96-4af6-469a-f51d-e0aa28d31fbc"
      },
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[110, 420, 930, 1640, 2550]"
            ]
          },
          "metadata": {},
          "execution_count": 63
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "somatorio = 0\n",
        "for x in outro_mapa.collect():\n",
        "  somatorio += x\n",
        "print(somatorio)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-4gXQXR20uaA",
        "outputId": "5b0bf4b2-923c-4c89-be33-781582477de4"
      },
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "9735015\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Podemos agora utilizar um outro conjunto de dados formado por palavras. Com isso, variável paralelo agora faz referência ao RDD:"
      ],
      "metadata": {
        "id": "FQgH8-vIxLER"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "paralelo = spark_context.parallelize([\"distribuida\", \"distribuida\", \"spark\", \"rdd\", \"spark\", \"spark\"])"
      ],
      "metadata": {
        "id": "kiwO9-KxxPsk"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "O próximo passo é implementar uma função lambda para processar este conjunto de dados. Neste caso, iremos associar o número 1 a uma palavra, formando um conjunto de chave;valor:"
      ],
      "metadata": {
        "id": "Hl4RJ-tyxdGE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "funcao_lambda = lambda x:(x, 1)"
      ],
      "metadata": {
        "id": "sEJ5QFwwxvYe"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Feito isso, agora podemos aplicar o MapReduce para organizar os dados e depois reduzir eles por chave, somando a quantidade de vezes que esta chave é encontrada no conjunto inicial. Resumidamente a função reduceByKey soma as ocorrência e as agrupa pela chave - no caso, pelas palavras. Por fim, coletamos um novo conjunto de dados onde teremos também uma chave(nome);valor(vezes em que a chave aparece no conjunto que foi processado)."
      ],
      "metadata": {
        "id": "y80d165px1Kw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from operator import add\n",
        "mapa = paralelo.map(funcao_lambda).reduceByKey(add).collect()"
      ],
      "metadata": {
        "id": "5JbXYVCeybsm"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Agora podemos percorrer a lista mapa para visualizar cada par formado pela palavra e sua respectiva ocorrência:"
      ],
      "metadata": {
        "id": "iCOpatuhzSF5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for (w, c) in mapa:\n",
        "  print(\"{}: {}\".format(w,c))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2HfsQwqQyqve",
        "outputId": "85b511a9-0f8c-475e-eeb3-6c1f82d8962e"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "distribuida: 2\n",
            "spark: 3\n",
            "rdd: 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Por fim, então finalizamos o contexto do spark"
      ],
      "metadata": {
        "id": "QH-Et0Pay9zY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "spark_context.stop()"
      ],
      "metadata": {
        "id": "9HHVXCRFzAi5"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Prática 7 - MapReduce com transformação e ação"
      ],
      "metadata": {
        "id": "UH8XBCat3I9f"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Considerando que o SparkContext já foi instanciado anteriormente, então o próximo passo é fornecer uma lista de entrada e transformá-la em um RDD do spark:"
      ],
      "metadata": {
        "id": "RoiEVs3b3N9S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lista = [1,2,3,4,5,3]\n",
        "lista_rdd = spark_context.parallelize(lista)"
      ],
      "metadata": {
        "id": "PrDYH_NU3XCu"
      },
      "execution_count": 83,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Agora podemos executar uma ação, como por exemplo de contar os elementos do RDD:"
      ],
      "metadata": {
        "id": "Sjw7QYf-3hUf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lista_rdd.count()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TfpBSEqF3nq0",
        "outputId": "112179bd-f42a-4dce-9212-ec9f13369903"
      },
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "6"
            ]
          },
          "metadata": {},
          "execution_count": 84
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Além disso, podemos criar uma função lambda que recebe um número como parâmetro e retorna um par formado pelo número do parâmetro e pelo mesmo número multiplicado por 10:"
      ],
      "metadata": {
        "id": "U7w6U8ar3pxR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "func_parOrdenado = lambda numero: (numero, numero*10)\n",
        "func_parOrdenadoQuadrado = lambda numero: (numero, numero**2)"
      ],
      "metadata": {
        "id": "HqMPrOj-31oD"
      },
      "execution_count": 85,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Com a função criada, podemos aplicar a transformação flatMap com a ação collect da função lambda para a lista_rdd:"
      ],
      "metadata": {
        "id": "360-zZAr37xA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lista_rdd.flatMap(func_parOrdenado).collect()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H-FngOOB4D2r",
        "outputId": "fcaec7b6-df70-4840-8096-b6213055c205"
      },
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[1, 10, 2, 20, 3, 30, 4, 40, 5, 50, 3, 30]"
            ]
          },
          "metadata": {},
          "execution_count": 86
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Além disso, podemos ainda aplicar a transformação map com a ação collect da função lambda para a lista_rdd e desta forma produzir uma saída de conjunto de chave;valor dentro do vetor principal:"
      ],
      "metadata": {
        "id": "9Zk0aUbW4Wqo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lista_rdd.map(func_parOrdenado).collect()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xI9rpZT44iel",
        "outputId": "46c3a983-2295-4627-9d64-f9844d71944d"
      },
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(1, 10), (2, 20), (3, 30), (4, 40), (5, 50), (3, 30)]"
            ]
          },
          "metadata": {},
          "execution_count": 87
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "lista_rdd.map(func_parOrdenadoQuadrado).collect()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9s-D8CMx5usq",
        "outputId": "c2a50f29-f3bc-4aa2-a0d7-5c57eeede590"
      },
      "execution_count": 88,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(1, 1), (2, 4), (3, 9), (4, 16), (5, 25), (3, 9)]"
            ]
          },
          "metadata": {},
          "execution_count": 88
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Por fim, encerramos o contexto do spark:"
      ],
      "metadata": {
        "id": "MMbpOSyt4qol"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "spark_context.stop()"
      ],
      "metadata": {
        "id": "q7SEXqWH4nPb"
      },
      "execution_count": 89,
      "outputs": []
    }
  ]
}